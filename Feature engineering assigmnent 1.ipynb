{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b603d735-141e-40ce-a488-f8d28fe9e46a",
   "metadata": {},
   "source": [
    "ans1:\n",
    "\n",
    "\n",
    "Min-Max scaling, also known as Min-Max normalization or feature scaling, is a method used in data preprocessing to scale numeric features of a dataset to a specific range, usually between 0 and 1. The purpose of Min-Max scaling is to standardize the range of independent variables, making it easier to compare and analyze them. This scaling method is particularly useful when working with machine learning algorithms that are sensitive to the scale of input features, such as gradient descent-based algorithms.\n",
    "\n",
    "The formula for Min-Max scaling is given by:\n",
    "\n",
    "\\[ X_{\\text{scaled}} = \\frac{X - \\text{min}(X)}{\\text{max}(X) - \\text{min}(X)} \\]\n",
    "\n",
    "where \\(X\\) is the original feature, \\(X_{\\text{scaled}}\\) is the scaled feature, \\(\\text{min}(X)\\) is the minimum value of the feature, and \\(\\text{max}(X)\\) is the maximum value of the feature.\n",
    "\n",
    "Here's a simple example to illustrate Min-Max scaling:\n",
    "\n",
    "Suppose you have a dataset with a feature \\(X\\) representing the ages of individuals, and the ages range from 20 to 40. You want to scale this feature to a range between 0 and 1 using Min-Max scaling.\n",
    "\n",
    "Original ages:\n",
    "\\[ X = [20, 25, 30, 35, 40] \\]\n",
    "\n",
    "Now, apply Min-Max scaling:\n",
    "\n",
    "\\[ X_{\\text{scaled}} = \\frac{X - \\text{min}(X)}{\\text{max}(X) - \\text{min}(X)} \\]\n",
    "\n",
    "\\[ X_{\\text{scaled}} = \\frac{[20, 25, 30, 35, 40] - 20}{40 - 20} \\]\n",
    "\n",
    "\\[ X_{\\text{scaled}} = \\frac{[0, 5, 10, 15, 20]}{20} \\]\n",
    "\n",
    "\\[ X_{\\text{scaled}} = [0.00, 0.25, 0.50, 0.75, 1.00] \\]\n",
    "\n",
    "So, the Min-Max scaled ages would be \\([0.00, 0.25, 0.50, 0.75, 1.00]\\), ensuring that all values are now within the desired range of 0 to 1."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d83bd620-869c-4f88-ad0f-cbe1bad37200",
   "metadata": {},
   "source": [
    "#ans2:\n",
    "\n",
    "The Unit Vector technique, also known as Unit Vector Scaling or Vector normalization, is a method used in feature scaling. The goal of feature scaling is to standardize or normalize the range of independent variables or features of a dataset. The Unit Vector technique specifically transforms the data such that each feature vector has a magnitude of 1. This is achieved by dividing each feature value by the Euclidean norm (magnitude) of the entire feature vector.\n",
    "\n",
    "Mathematically, for a feature vector X = [x1, x2, ..., xn], the unit vector u is given by:\n",
    "\n",
    "\\[ u = \\frac{X}{\\|X\\|} \\]\n",
    "\n",
    "where \\|X\\| represents the Euclidean norm of the vector X.\n",
    "\n",
    "On the other hand, Min-Max scaling, also known as Min-Max normalization, scales the features to a specific range, typically between 0 and 1. The formula for Min-Max scaling is:\n",
    "\n",
    "\\[ X_{\\text{scaled}} = \\frac{X - \\text{min}(X)}{\\text{max}(X) - \\text{min}(X)} \\]\n",
    "\n",
    "Now, let's illustrate the application of the Unit Vector technique with an example:\n",
    "\n",
    "Suppose we have a dataset with two features: \\(X_1 = [3, 4]\\) and \\(X_2 = [1, 2]\\). We want to scale these features using the Unit Vector technique.\n",
    "\n",
    "1. Calculate the Euclidean norm for each feature vector:\n",
    "   \\[ \\|X_1\\| = \\sqrt{3^2 + 4^2} = 5 \\]\n",
    "   \\[ \\|X_2\\| = \\sqrt{1^2 + 2^2} = \\sqrt{5} \\]\n",
    "\n",
    "2. Scale each feature vector by dividing it by its respective Euclidean norm:\n",
    "   \\[ u_1 = \\frac{X_1}{\\|X_1\\|} = \\frac{[3, 4]}{5} = [0.6, 0.8] \\]\n",
    "   \\[ u_2 = \\frac{X_2}{\\|X_2\\|} = \\frac{[1, 2]}{\\sqrt{5}} \\approx [0.45, 0.89] \\]\n",
    "\n",
    "So, after applying the Unit Vector technique, the scaled feature vectors are \\(u_1 = [0.6, 0.8]\\) and \\(u_2 = [0.45, 0.89]\\).\n",
    "\n",
    "In contrast, if we were to apply Min-Max scaling to the same dataset, the scaled feature vectors would be different, and the values would be within a specified range, typically 0 to 1.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bcc55c68-b1a2-4783-b9f7-6338590e93ad",
   "metadata": {},
   "source": [
    "#asn3:\n",
    "\n",
    "Principal Component Analysis (PCA) is a method used for dimensionality reduction in data analysis and machine learning. It transforms high-dimensional data into a lower-dimensional space by identifying the principal components, which are the directions that capture the most variance. PCA involves standardizing the data, computing the covariance matrix, calculating eigenvectors and eigenvalues, selecting principal components, and projecting the data onto these components. It is useful for simplifying data while retaining essential information. In a simple example with height and weight data, PCA reduces the two-dimensional dataset to one dimension, capturing the main trends in the data.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80e7089f-4722-41f6-9d05-53281fe8fe04",
   "metadata": {},
   "source": [
    "#ans4:\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that is commonly used for feature extraction in machine learning and data analysis. Feature extraction involves transforming the original set of features into a new set of features, ideally with reduced dimensionality while retaining as much relevant information as possible.\n",
    "\n",
    "PCA works by finding the principal components in the data, which are linear combinations of the original features. These principal components capture the maximum variance in the data. The first principal component explains the most variance, the second principal component (orthogonal to the first) explains the second most, and so on. By selecting a subset of these principal components, one can achieve dimensionality reduction.\n",
    "\n",
    "Here's how PCA can be used for feature extraction:\n",
    "\n",
    "1. **Covariance Matrix:**\n",
    "   - Given a dataset with n-dimensional features, calculate the covariance matrix.\n",
    "   - The covariance matrix represents the relationships between different features.\n",
    "\n",
    "2. **Eigendecomposition:**\n",
    "   - Perform eigendecomposition on the covariance matrix to obtain eigenvectors and eigenvalues.\n",
    "   - Eigenvectors represent the directions of maximum variance, and eigenvalues represent the magnitude of variance in those directions.\n",
    "\n",
    "3. **Select Principal Components:**\n",
    "   - Sort the eigenvectors based on their corresponding eigenvalues in descending order.\n",
    "   - Choose the top k eigenvectors (principal components) where k is the desired reduced dimensionality.\n",
    "\n",
    "4. **Transform Data:**\n",
    "   - Multiply the original data by the selected eigenvectors to obtain the new feature space.\n",
    "\n",
    "Let's illustrate this concept with an example:\n",
    "\n",
    "Suppose you have a dataset with two features, X1 (height in inches) and X2 (weight in pounds). You want to reduce the dimensionality to one principal component.\n",
    "\n",
    "- **Step 1: Covariance Matrix**\n",
    "  ```\n",
    "  Covariance Matrix:\n",
    "  | Cov(X1, X1)  Cov(X1, X2) |\n",
    "  | Cov(X2, X1)  Cov(X2, X2) |\n",
    "  ```\n",
    "\n",
    "- **Step 2: Eigendecomposition**\n",
    "  - Calculate eigenvalues (Î») and eigenvectors (v) from the covariance matrix.\n",
    "\n",
    "- **Step 3: Select Principal Components**\n",
    "  - If the first principal component has a higher eigenvalue, choose it for dimensionality reduction.\n",
    "\n",
    "- **Step 4: Transform Data**\n",
    "  - Multiply the original data by the selected eigenvector.\n",
    "\n",
    "In this example, the new feature (principal component) will be a linear combination of the original features that captures the most significant variation in the data. This process helps in reducing the dimensionality of the dataset while preserving important information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b86d764-17a4-4efb-b3f1-b83f04830c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      "   price  rating  delivery_time\n",
      "0     10     4.5             20\n",
      "1     20     3.0             30\n",
      "2     15     4.8             25\n",
      "3     25     3.5             35\n",
      "4     30     5.0             40\n",
      "\n",
      "Scaled Dataset:\n",
      "   price  rating  delivery_time\n",
      "0   0.00    0.75           0.00\n",
      "1   0.50    0.00           0.50\n",
      "2   0.25    0.90           0.25\n",
      "3   0.75    0.25           0.75\n",
      "4   1.00    1.00           1.00\n"
     ]
    }
   ],
   "source": [
    "#asn5:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'price': [10, 20, 15, 25, 30],\n",
    "    'rating': [4.5, 3.0, 4.8, 3.5, 5.0],\n",
    "    'delivery_time': [20, 30, 25, 35, 40]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Min-Max Scaling function\n",
    "def min_max_scaling(dataset, features_to_scale):\n",
    "    for feature in features_to_scale:\n",
    "        min_val = min(dataset[feature])\n",
    "        max_val = max(dataset[feature])\n",
    "        dataset[feature] = (dataset[feature] - min_val) / (max_val - min_val)\n",
    "    return dataset\n",
    "\n",
    "# Features to scale\n",
    "features_to_scale = ['price', 'rating', 'delivery_time']\n",
    "\n",
    "# Apply Min-Max Scaling\n",
    "df_scaled = min_max_scaling(df.copy(), features_to_scale)\n",
    "\n",
    "# Display the original and scaled dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(df)\n",
    "print(\"\\nScaled Dataset:\")\n",
    "print(df_scaled)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea853924-3726-4353-b9eb-d6bb2b4aa5d8",
   "metadata": {},
   "source": [
    "#ans6:\n",
    "\n",
    "\n",
    "Principal Component Analysis (PCA) is a technique commonly used for dimensionality reduction in datasets with many features. In the context of building a model to predict stock prices, where the dataset includes various features like company financial data and market trends, PCA can be employed to streamline the data and capture the most critical information.\n",
    "\n",
    "Here's a step-by-step explanation of how PCA can be applied to reduce the dimensionality of the dataset:\n",
    "\n",
    "1. **Understand the Data:**\n",
    "   - Begin by understanding the structure and nature of the dataset. Identify the features that contribute the most to the variability in the data.\n",
    "\n",
    "2. **Standardize the Data:**\n",
    "   - Standardize the dataset by scaling each feature to have a mean of 0 and a standard deviation of 1. This is crucial for PCA, as it is sensitive to the scale of the features.\n",
    "\n",
    "3. **Compute Covariance Matrix:**\n",
    "   - Calculate the covariance matrix of the standardized dataset. The covariance matrix summarizes the relationships between different features and helps identify the directions in which the data varies the most.\n",
    "\n",
    "4. **Compute Eigenvectors and Eigenvalues:**\n",
    "   - Determine the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the principal components, and eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "5. **Sort Eigenvectors by Eigenvalues:**\n",
    "   - Sort the eigenvectors based on their corresponding eigenvalues in descending order. The higher the eigenvalue, the more variance is explained by the corresponding eigenvector.\n",
    "\n",
    "6. **Select Principal Components:**\n",
    "   - Choose the top k eigenvectors that correspond to the k largest eigenvalues to form the principal components. This is where the dimensionality reduction occurs. The value of k is determined based on the desired level of variance to retain in the data.\n",
    "\n",
    "7. **Project Data onto Principal Components:**\n",
    "   - Project the original dataset onto the selected principal components. This results in a new dataset with reduced dimensionality.\n",
    "\n",
    "8. **Train Model on Reduced Dataset:**\n",
    "   - Use the reduced dataset to train your stock price prediction model. The reduced dataset retains most of the variability in the original data, as it is composed of the principal components that capture the most significant information.\n",
    "\n",
    "By applying PCA, you can reduce the dimensionality of the dataset while retaining most of the important information, leading to potentially more efficient and effective modeling for predicting stock prices. Keep in mind that the choice of the number of principal components (k) is a trade-off between dimensionality reduction and the amount of variance retained. Cross-validation techniques can help determine an appropriate value for k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d68ef810-ac1c-4621-aad7-9157b42189fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#asn7:\n",
    "\n",
    "data=[1, 5, 10, 15, 20]\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "normalised_data= MinMaxScaler()\n",
    "list(normalised_data.fit_transform([data]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b76b5c-a4e3-4bd2-a1d0-ef8323197b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80250efb-1867-4658-9ae5-c76433913217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b20814-bbde-44be-9856-2824f37adf4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b88d9f-b5d4-47d2-8bf5-bbeea6f6231f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88f0619-f935-427b-b626-989ba8d30c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b22e9-5f47-44ad-bb22-f37105e16e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd9a81-7000-4b8a-9b8d-830862e8fa5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57edd1a-86de-40c7-b0d8-fbf46c6986dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c199e6dc-df85-4cf4-8371-055c385c5c36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550f8391-3984-4827-ab93-3d177c4cb607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
