{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55e20c40-4207-43d7-b2c3-fd0b6657721a",
   "metadata": {},
   "source": [
    "# ans 1:\n",
    "\n",
    "To find the probability that an employee is a smoker given that he/she uses the health insurance plan, you can use the conditional probability formula:\n",
    "\n",
    "\\[ P(\\text{Smoker | Uses Health Insurance}) = \\frac{P(\\text{Smoker and Uses Health Insurance})}{P(\\text{Uses Health Insurance})} \\]\n",
    "\n",
    "Let's denote the events as follows:\n",
    "\n",
    "\\( A \\): Employee uses the health insurance plan\n",
    "\\( B \\): Employee is a smoker\n",
    "\n",
    "The information given is:\n",
    "\n",
    "\\[ P(A) = 0.70 \\] (70% of employees use the health insurance plan)\n",
    "\n",
    "\\[ P(B | A) = 0.40 \\] (40% of employees who use the plan are smokers)\n",
    "\n",
    "Now, plug these values into the formula:\n",
    "\n",
    "\\[ P(B | A) = \\frac{P(B \\cap A)}{P(A)} \\]\n",
    "\n",
    "\\[ P(\\text{Smoker | Uses Health Insurance}) = \\frac{0.40 \\times 0.70}{0.70} \\]\n",
    "\n",
    "\\[ P(\\text{Smoker | Uses Health Insurance}) = 0.40 \\]\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.40 or 40%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62180e1a-8409-4141-8345-d91b02a90772",
   "metadata": {},
   "source": [
    "# asn 2:\n",
    "\n",
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are both variants of the Naive Bayes algorithm used for classification tasks, but they differ in terms of the types of data they are suitable for and their underlying assumptions.\n",
    "\n",
    "1. **Bernoulli Naive Bayes:**\n",
    "   - **Data Type:** Bernoulli Naive Bayes is suitable for binary data, where features are binary-valued (0 or 1), indicating the presence or absence of a particular feature.\n",
    "   - **Assumption:** It assumes that features are binary and that the presence or absence of each feature is equally important for classification.\n",
    "\n",
    "2. **Multinomial Naive Bayes:**\n",
    "   - **Data Type:** Multinomial Naive Bayes is appropriate for data that can be represented as counts or frequencies of events, typically in the form of integer counts (e.g., word counts in document classification).\n",
    "   - **Assumption:** It assumes that the features follow a multinomial distribution, which means they represent the occurrences or frequencies of events.\n",
    "\n",
    "In summary, the main difference lies in the type of data they are designed to handle. Bernoulli Naive Bayes is suitable for binary data, while Multinomial Naive Bayes is more appropriate for categorical data represented by counts or frequencies. The choice between them depends on the nature of your data and the assumptions that align with your specific classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5586fbc-2846-4b44-ba4f-a0675d3f6ef5",
   "metadata": {},
   "source": [
    "# asn 3:\n",
    "Bernoulli Naive Bayes, like other Naive Bayes variants, does not explicitly handle missing values. However, you can preprocess your data to handle missing values before applying the algorithm. Here are a few common approaches:\n",
    "\n",
    "1. **Imputation:** Replace missing values with a specific value, such as the mode (most frequent value) or mean of the feature. In the case of binary features used in Bernoulli Naive Bayes, you might replace missing values with either 0 or 1, depending on the distribution of the feature.\n",
    "\n",
    "2. **Meaningful Imputation:** For binary features, you might consider replacing missing values with a value that makes sense in the context of your data. For example, if the feature represents the presence or absence of a specific attribute, you might choose to impute missing values with 0 (absence) if that's more appropriate.\n",
    "\n",
    "3. **Create a Separate Category:** You can also treat missing values as a separate category and encode them as such in your dataset. This approach works well when missing values carry some information or represent a meaningful absence of data.\n",
    "\n",
    "4. **Model-based Imputation:** Utilize other predictive models to estimate missing values based on the observed values of other features. Once missing values are imputed, you can proceed with Bernoulli Naive Bayes as usual.\n",
    "\n",
    "Regardless of the approach chosen, it's essential to carefully consider the implications of missing values on your dataset and choose a method that aligns with the assumptions of your analysis and the characteristics of your data. Additionally, preprocessing steps should be applied consistently across training and testing datasets to ensure fair and accurate evaluation of your model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f057884-4504-4334-83dc-15fc9150fdcf",
   "metadata": {},
   "source": [
    "# asn 4:\n",
    "Yes, Gaussian Naive Bayes (GNB) can be used for multi-class classification tasks. Naive Bayes classifiers, including the Gaussian variant, are inherently designed for both binary and multi-class classification problems.\n",
    "\n",
    "In the case of Gaussian Naive Bayes, the assumption is that the features within each class follow a Gaussian (normal) distribution. Despite its \"naive\" assumption of feature independence, it often performs well in practice, especially when the features are approximately normally distributed.\n",
    "\n",
    "For multi-class classification, GNB extends the basic concept by considering each class independently and assigning the class with the highest posterior probability given the input features. The decision rule is based on maximizing the class conditional probabilities.\n",
    "\n",
    "In summary, Gaussian Naive Bayes is a valid choice for multi-class classification tasks, and it can be particularly effective when the underlying assumptions align well with the characteristics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c08d1919-8de4-4a5c-8e3a-1f0bfca39cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes:\n",
      "Accuracy: 0.8839380364047911\n",
      "Precision: 0.8813357185450209\n",
      "Recall: 0.815223386651958\n",
      "F1 Score: 0.8469914040114614\n",
      "\n",
      "\n",
      "Multinomial Naive Bayes:\n",
      "Accuracy: 0.7863496180326323\n",
      "Precision: 0.7323628219484882\n",
      "Recall: 0.7214561500275786\n",
      "F1 Score: 0.7268685746040567\n",
      "\n",
      "\n",
      "Gaussian Naive Bayes:\n",
      "Accuracy: 0.8217730830896915\n",
      "Precision: 0.7004440855874041\n",
      "Recall: 0.9569773855488142\n",
      "F1 Score: 0.8088578088578089\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ans 5:\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "names = [f\"feature_{i}\" for i in range(57)] + ['is_spam']\n",
    "data = pd.read_csv(url, names=names)\n",
    "\n",
    "# Split the dataset\n",
    "X = data.drop('is_spam', axis=1)\n",
    "y = data['is_spam']\n",
    "\n",
    "# Implement and evaluate Bernoulli Naive Bayes\n",
    "bernoulli_nb = BernoulliNB()\n",
    "y_pred_bernoulli = cross_val_predict(bernoulli_nb, X, y, cv=10)\n",
    "bernoulli_scores = cross_val_score(bernoulli_nb, X, y, cv=10)\n",
    "\n",
    "# Implement and evaluate Multinomial Naive Bayes\n",
    "multinomial_nb = MultinomialNB()\n",
    "y_pred_multinomial = cross_val_predict(multinomial_nb, X, y, cv=10)\n",
    "multinomial_scores = cross_val_score(multinomial_nb, X, y, cv=10)\n",
    "\n",
    "# Implement and evaluate Gaussian Naive Bayes\n",
    "gaussian_nb = GaussianNB()\n",
    "y_pred_gaussian = cross_val_predict(gaussian_nb, X, y, cv=10)\n",
    "gaussian_scores = cross_val_score(gaussian_nb, X, y, cv=10)\n",
    "\n",
    "# Report performance metrics\n",
    "def report_metrics(name, y_pred, scores):\n",
    "    print(f\"{name} Naive Bayes:\")\n",
    "    print(f\"Accuracy: {scores.mean()}\")\n",
    "    print(f\"Precision: {precision_score(y, y_pred)}\")\n",
    "    print(f\"Recall: {recall_score(y, y_pred)}\")\n",
    "    print(f\"F1 Score: {f1_score(y, y_pred)}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "report_metrics(\"Bernoulli\", y_pred_bernoulli, bernoulli_scores)\n",
    "report_metrics(\"Multinomial\", y_pred_multinomial, multinomial_scores)\n",
    "report_metrics(\"Gaussian\", y_pred_gaussian, gaussian_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0679752d-164d-429f-ae5b-37a41bbe1aed",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbb19d6-87a3-4589-b400-90f79a4192df",
   "metadata": {},
   "source": [
    "**Discussion (Continued):**\n",
    "\n",
    "In the discussion, analyze the results obtained from the three Naive Bayes variants:\n",
    "\n",
    "- **Bernoulli Naive Bayes:**\n",
    "  - This variant assumes that features are binary, making it suitable for binary data like the presence or absence of words in spam emails.\n",
    "  - Discuss if the binary assumption aligns well with the dataset and if this influenced the performance metrics.\n",
    "\n",
    "- **Multinomial Naive Bayes:**\n",
    "  - This variant is commonly used for discrete data, such as word counts in text classification.\n",
    "  - Examine whether the assumptions of this model match the characteristics of the Spambase dataset.\n",
    "\n",
    "- **Gaussian Naive Bayes:**\n",
    "  - Gaussian Naive Bayes is designed for continuous data and assumes a Gaussian distribution of features.\n",
    "  - Discuss how well this assumption holds for the dataset and if it impacted the classifier's performance.\n",
    "\n",
    "Compare the metrics such as accuracy, precision, recall, and F1 score for each variant. Identify any patterns or trends in the results.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "Summarize the key findings and insights:\n",
    "\n",
    "- Highlight which Naive Bayes variant performed the best in terms of the specified metrics.\n",
    "- Discuss possible reasons for the observed performance differences, considering the dataset characteristics and the assumptions of each variant.\n",
    "- Mention any limitations observed during the analysis, such as sensitivity to feature assumptions or potential overfitting.\n",
    "\n",
    "**Suggestions for Future Work:**\n",
    "\n",
    "Provide recommendations for future work based on your observations:\n",
    "\n",
    "- Explore feature engineering techniques to enhance the performance of Naive Bayes models.\n",
    "- Consider hyperparameter tuning to optimize the models further.\n",
    "- Experiment with other machine learning algorithms to compare their performance with Naive Bayes.\n",
    "- Investigate ensemble methods or hybrid models for potentially improved classification accuracy.\n",
    "\n",
    "By concluding with these suggestions, you provide a roadmap for further research and improvement in spam email classification using machine learning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fd4c28-47ac-4c2c-9026-1e49029cfa9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
