{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05a3d4bd-f408-4b41-b7ee-9a882cd1c49b",
   "metadata": {},
   "source": [
    "Q1. **How does bagging reduce overfitting in decision trees?**\n",
    "   Bagging (Bootstrap Aggregating) reduces overfitting in decision trees by creating an ensemble of multiple trees, each trained on a different subset of the original training data. The process involves bootstrapping, where random samples with replacement are drawn from the dataset to train individual trees. By averaging or combining the predictions of these trees, the ensemble tends to generalize better to unseen data, reducing the risk of overfitting.\n",
    "\n",
    "Q2. **What are the advantages and disadvantages of using different types of base learners in bagging?**\n",
    "   - *Advantages:* Bagging can improve the performance of various base learners. It tends to work well with unstable or high-variance models, such as decision trees. It can also enhance the robustness of weak learners, leading to a more accurate and stable ensemble.\n",
    "   - *Disadvantages:* Bagging may not provide significant benefits if the base learner is already a low-variance model. In some cases, it might lead to a minimal improvement or even degradation in performance. Additionally, bagging may increase computational complexity, especially when using complex base learners.\n",
    "\n",
    "Q3. **How does the choice of base learner affect the bias-variance tradeoff in bagging?**\n",
    "   Bagging has a tendency to reduce variance by averaging the predictions of multiple base learners. The choice of a base learner affects the bias-variance tradeoff in the sense that if the base learner is a high-bias model (e.g., a shallow decision tree), bagging might not be as effective in reducing bias. On the other hand, if the base learner is a low-bias, high-variance model (e.g., a deep decision tree), bagging can significantly reduce variance, leading to a more balanced bias-variance tradeoff.\n",
    "\n",
    "Q4. **Can bagging be used for both classification and regression tasks? How does it differ in each case?**\n",
    "   Yes, bagging can be used for both classification and regression tasks. In both cases, it involves creating an ensemble of base learners trained on different subsets of the data. The primary difference lies in how the predictions are combined:\n",
    "   - **Classification:** The predictions of base learners are often combined through a majority or weighted voting scheme to determine the final class label.\n",
    "   - **Regression:** The predictions are typically averaged to produce the final regression output.\n",
    "\n",
    "Q5. **What is the role of ensemble size in bagging? How many models should be included in the ensemble?**\n",
    "   The ensemble size in bagging plays a crucial role in determining the performance. Initially, increasing the ensemble size tends to improve the model's performance by reducing variance. However, there is a point of diminishing returns, and adding more models beyond a certain point might not significantly improve performance while increasing computational resources. The optimal ensemble size can vary depending on the dataset and the base learner, and it is often determined through cross-validation or other model evaluation techniques.\n",
    "\n",
    "Q6. **Can you provide an example of a real-world application of bagging in machine learning?**\n",
    "   One real-world application of bagging is in the field of finance for credit scoring. In this scenario, bagging can be employed to create an ensemble of decision trees to predict the creditworthiness of individuals based on various features such as income, credit history, and debt. By combining predictions from multiple trees trained on different subsets of the dataset, the model becomes more robust and less susceptible to overfitting, leading to more accurate credit risk assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb355ecd-2818-4f4d-9eae-94deba97fc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Bagging Classifier: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Decision Tree classifier as the base learner\n",
    "base_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Create a Bagging Classifier with 100 base learners (trees)\n",
    "bagging_classifier = BaggingClassifier(base_classifier, n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the Bagging Classifier on the training data\n",
    "bagging_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = bagging_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the Bagging Classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy of Bagging Classifier: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660318a2-cfa2-44ce-8ff3-a71fe737788b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
