{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5704db64-8b69-42a4-892b-4820f97d93ad",
   "metadata": {},
   "source": [
    "# ans 1:\n",
    "\n",
    "Polynomial functions and kernel functions are both relevant concepts in machine learning, particularly in the context of kernelized methods such as Support Vector Machines (SVMs). Let's break down their relationship:\n",
    "\n",
    "1. **Polynomial Functions:**\n",
    "   - A polynomial function is a mathematical function composed of one or more terms, each involving variables raised to non-negative integer powers and multiplied by coefficients.\n",
    "   - In the context of machine learning, polynomial functions are often used as basis functions for feature transformations. For example, given a feature vector `[x1, x2]`, a polynomial transformation of degree 2 might map it to `[1, x1, x2, x1^2, x1*x2, x2^2]`.\n",
    "\n",
    "2. **Kernel Functions:**\n",
    "   - Kernel functions play a crucial role in kernelized machine learning algorithms, such as the kernel trick used in SVMs. They implicitly map input data into a higher-dimensional space without explicitly calculating the transformed feature vectors.\n",
    "   - The kernel trick allows algorithms to operate in a high-dimensional space without explicitly computing the transformed feature vectors, which can be computationally expensive.\n",
    "\n",
    "3. **Relationship:**\n",
    "   - Polynomial kernel functions are a specific type of kernel function used in SVMs. The polynomial kernel is defined as \\(K(x, y) = (x \\cdot y + c)^d\\), where \\(d\\) is the degree of the polynomial and \\(c\\) is a constant.\n",
    "   - The polynomial kernel implicitly computes the dot product in a higher-dimensional space, capturing complex relationships between features.\n",
    "\n",
    "In summary, polynomial functions are used as basis functions for feature transformations, and polynomial kernel functions are a specific type of kernel function that incorporates the idea of polynomial transformations. The kernel trick enables machine learning algorithms to efficiently work in higher-dimensional spaces without explicitly calculating the transformed feature vectors, making it particularly useful in scenarios where the transformed space is infinite-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bd1af79-d355-4d68-947f-fd45fe10da51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# ans 2:\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier with a polynomial kernel\n",
    "svm_classifier = SVC(kernel='poly', degree=3, C=1.0)\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7865f539-4570-489a-a398-a554abe63d01",
   "metadata": {},
   "source": [
    "## ans3:\n",
    "\n",
    "In Support Vector Regression (SVR), epsilon (often denoted as ε) is a hyperparameter that determines the width of the epsilon-insensitive tube around the predicted values. The epsilon-insensitive tube is a range within which errors are not penalized, and only errors outside this tube contribute to the loss function.\n",
    "\n",
    "As you increase the value of epsilon in SVR, the width of the epsilon-insensitive tube also increases. This means that a larger margin is allowed for data points to fall within without incurring a penalty. As a result, the SVR model becomes more tolerant to errors, and a greater number of data points may fall within the tube without affecting the loss function.\n",
    "\n",
    "When more data points are allowed to be within the epsilon-insensitive tube, the likelihood of them becoming support vectors decreases. Support vectors are the data points that lie on the margin or within the tube boundaries. As epsilon increases, the margin widens, and fewer data points are treated as support vectors.\n",
    "\n",
    "In summary, increasing the value of epsilon in SVR tends to decrease the number of support vectors by allowing more data points to be within the wider epsilon-insensitive tube without contributing significantly to the loss function. The choice of epsilon should be carefully considered based on the specific characteristics of the data and the desired trade-off between model complexity and accuracy.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6a6838-ce7b-480a-857c-d4a0b487a843",
   "metadata": {},
   "source": [
    "## ans4:\n",
    "Support Vector Regression (SVR) is a machine learning algorithm that uses support vector machines to perform regression tasks. The performance of SVR can be significantly influenced by the choice of kernel function and the values assigned to the parameters: C, epsilon, and gamma. Let's discuss each parameter:\n",
    "\n",
    "1. **Kernel Function:**\n",
    "   - The kernel function determines the type of hyperplane used for regression. Common choices include linear, polynomial, radial basis function (RBF), and sigmoid.\n",
    "   - **Example:**\n",
    "     - Linear Kernel (`kernel='linear'`): Suitable for linear relationships between features.\n",
    "     - RBF Kernel (`kernel='rbf'`): Effective when dealing with non-linear relationships. It's a good default choice, but the performance can be sensitive to the gamma parameter.\n",
    "\n",
    "2. **C Parameter:**\n",
    "   - The C parameter controls the trade-off between having a smooth decision boundary and classifying the training points correctly.\n",
    "   - **Effect:**\n",
    "     - Small C values lead to a smooth decision surface, allowing more training points to be ignored or \"misclassified.\"\n",
    "     - Large C values result in a more rigid decision boundary that classifies all training points correctly, but it may lead to overfitting.\n",
    "   - **Example:**\n",
    "     - Increase C when you want to have a more accurate fit to the training data and are not concerned about overfitting.\n",
    "\n",
    "3. **Epsilon Parameter (ε):**\n",
    "   - The epsilon parameter defines the margin of tolerance where no penalty is given to errors.\n",
    "   - **Effect:**\n",
    "     - Larger values of epsilon allow for a wider margin of error in the training data.\n",
    "     - Smaller epsilon values lead to a narrower margin, requiring the model to fit the training data more closely.\n",
    "   - **Example:**\n",
    "     - Increase epsilon if you want the model to be less sensitive to errors in the training data.\n",
    "\n",
    "4. **Gamma Parameter:**\n",
    "   - The gamma parameter defines how far the influence of a single training example reaches, with low values meaning \"far\" and high values meaning \"close.\"\n",
    "   - **Effect:**\n",
    "     - Small gamma values result in a wider influence, leading to a smoother decision boundary.\n",
    "     - Large gamma values make the decision boundary more dependent on the training data, potentially causing overfitting.\n",
    "   - **Example:**\n",
    "     - Increase gamma for complex, non-linear datasets, but be cautious about overfitting.\n",
    "\n",
    "**Summary:**\n",
    "- Choosing the right kernel depends on the nature of your data.\n",
    "- Adjusting C and epsilon can help control the balance between fitting the training data and generalizing to new data.\n",
    "- Gamma is crucial for controlling the influence of a single training example, and its optimal value depends on the dataset.\n",
    "\n",
    "It's crucial to perform hyperparameter tuning using techniques like cross-validation to find the optimal combination for your specific dataset and problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d7a9dbb5-e355-4688-aba9-a82f8f9b6193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of trained SVC is: 97.34%\n",
      "\n",
      "The r2_score of trained SVC is: 88.41%\n",
      "\n",
      "classification report is:\n",
      "\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96        67\n",
      "           1       0.98      0.98      0.98       121\n",
      "\n",
      "    accuracy                           0.97       188\n",
      "   macro avg       0.97      0.97      0.97       188\n",
      "weighted avg       0.97      0.97      0.97       188\n",
      "\n",
      "Best Hyperparameters: {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# ans5 :\n",
    "\n",
    "#importing necessary libraries:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# loading the dataset\"FLIGHTS\" \n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer=load_breast_cancer()\n",
    "\n",
    "#L Split the dataset into training and testing set:\n",
    "x=cancer.data\n",
    "y=cancer.target\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y, test_size=0.33, random_state=42)\n",
    "\n",
    "#Preprocess the data using any technique of your choice (e.g. scaling, normaliMation)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler= StandardScaler()\n",
    "x_train_scaled=scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "#Create an instance of the SVC classifier and train it on the training data\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "sv_classifier=SVC(kernel=\"linear\")\n",
    "sv_classifier.fit(x_train_scaled,y_train)\n",
    "y_pred = sv_classifier.predict(x_test_scaled)\n",
    "\n",
    "#use the trained classifier to predict the labels of the testing data\n",
    "from sklearn.metrics import accuracy_score,r2_score,classification_report\n",
    "print(\"The Accuracy of trained SVC is: {:.2f}%\".format(accuracy_score(y_test, y_pred) * 100))\n",
    "print(\"\\nThe r2_score of trained SVC is: {:.2f}%\".format(r2_score(y_test, y_pred) * 100))\n",
    "print(\"\\nclassification report is:\\n\\n\\n\",classification_report(y_test, y_pred))\n",
    "\n",
    "#Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV toimprove its performanc_\n",
    "\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto'], 'kernel': ['linear', 'rbf']}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid=GridSearchCV(SVC(), param_grid=param_grid,cv=5)\n",
    "grid.fit(x_train_scaled,y_train)\n",
    "best_params = grid.best_params_\n",
    "print(f'Best Hyperparameters: {best_params}')\n",
    "\n",
    "# Step 9: Train the tuned classifier on the entire dataset\n",
    "tuned_classifier = grid.best_estimator_\n",
    "tuned_classifier.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Step 10: Save the trained classifier to a file\n",
    "import pickle\n",
    "\n",
    "# Assuming tuned_classifier is already trained\n",
    "tuned_classifier.fit(x_train_scaled, y_train)  # Make sure to fit it again before saving\n",
    "\n",
    "# Save the trained classifier to a file using pickle\n",
    "with open('trained_classifier_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(tuned_classifier, model_file)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
